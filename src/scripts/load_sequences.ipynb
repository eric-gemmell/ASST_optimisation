{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cc068c",
   "metadata": {},
   "source": [
    "# Only run this if you need to tbh, you dont want rewrite the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371bdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# standard library modules\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import requests\n",
    "import xmltodict\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810cb6f9-774f-4f3e-8903-1f754f651420",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/pfam/PF05935/?page_size=200&extra_fields=sequence&taxonomy\"\n",
    "output_filename = '../data/annotated_sequences.json'\n",
    "installation_progress_filename = \"../data/url_progress.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172b16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/annotated_sequences.json', 'w') as f:\n",
    "    json.dump([],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f33f4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_additional_info(seq_data):\n",
    "    accession = seq_data[\"Accession_Interpro\"]\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{accession}.xml\"\n",
    "    response = requests.get(url)\n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "    if(response.status_code == 200):\n",
    "        seq_data[\"lineage\"] = xml_data['uniprot']['entry']['organism']['lineage'][\"taxon\"]\n",
    "        seq_data[\"Accession_RefSeq\"] = \"\"\n",
    "        seq_data[\"Accession_AlphaFoldDB\"] = \"\"\n",
    "        for reference in xml_data['uniprot']['entry']['dbReference']:\n",
    "            try:\n",
    "                if reference['@type'] == 'RefSeq' or reference['@type'] == 'AlphaFoldDB':\n",
    "                    id = reference['@id']\n",
    "                    seq_data[\"Accession_\"+reference[\"@type\"]] = id\n",
    "            except:\n",
    "                print(f\"\\n Failed to get info for {accession}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"FAILED RETRIEVING ADDITIONAL DATA FOR {accession}\")\n",
    "    return seq_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a13525ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Starting to load sequences and identifiers from scratch!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a0g4f9w9&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a0r9pem7&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a177p0b0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a1h2ghv5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a1q6n5n3&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a1y5mz88&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2a5daa7&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2d9qkl9&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2e4ust5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2g2hg94&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2r4hlz4&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a2t9qb45&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a316ua41&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a376qen5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a3a4k454&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 3000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a3g2h9u2&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 3200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a3r7tsf5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 3400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a3v8r2v2&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 3600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a416dvd1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 3800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a4p5w194&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 4000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a4v5npl0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 4200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a533tep0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 4400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a5h7d1y1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 4600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a5j0n8u2&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 4800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a5u0lt33&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 5000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a5w4bfg0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 5200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a5y8dni8&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 5400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a623ee72&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 5600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a659pjw9&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 5800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a6g5r5b7&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 6000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a6m3p3a5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 6200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a6y2xzk0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 6400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a726y563&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 6600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a736he16&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 6800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a748gkr7&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 7000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a7c5r6m0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 7200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a7k2ej42&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 7400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a7t8fg90&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 7600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a7x3kja6&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 7800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a7y7adm1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 8000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a842qeg1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 8200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a8e6wzi5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 8400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a917c2p9&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 8600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a936ufc0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 8800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a947elp9&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 9000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a957dkb7&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 9200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a960e8i5&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 9400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a9d2feg4&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 9600\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Aa0a9e5zel6&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 9800\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Ab6xue8&extra_fields=sequence&page_size=200&taxonomy=\n",
      "\n",
      " Failed to get info for A0A9P0S6Z2\n",
      "\n",
      " Failed to get info for A0A9P0S6Z2\n",
      "\n",
      " Failed to get info for A0A9P0S6Z2\n",
      "\n",
      " Failed to get info for A0A9P0SFN7\n",
      "\n",
      " Failed to get info for A0A9P0SFN7\n",
      "\n",
      " Failed to get info for A0A9P0SFN7\n",
      "\n",
      " Failed to get info for A0A9P3DZB6\n",
      "\n",
      " Failed to get info for A0A9P3DZB6\n",
      "\n",
      " Failed to get info for A0A9P3DZB6\n",
      "\n",
      " Failed to get info for A0A9P2N1E9\n",
      "\n",
      " Failed to get info for A0A9P2N1E9\n",
      "\n",
      " Failed to get info for A0A9P2N1E9\n",
      "\n",
      " Failed to get info for A0A9Q2Q9D5\n",
      "\n",
      " Failed to get info for A0A9Q2Q9D5\n",
      "\n",
      " Failed to get info for A0A9Q2Q9D5\n",
      "\n",
      " Failed to get info for A0A9Q6TFK3\n",
      "\n",
      " Failed to get info for A0A9Q6TFK3\n",
      "\n",
      " Failed to get info for A0A9Q6TFK3\n",
      "\n",
      " Failed to get info for A0A9Q6U0S5\n",
      "\n",
      " Failed to get info for A0A9Q6U0S5\n",
      "\n",
      " Failed to get info for A0A9Q6U0S5\n",
      "\n",
      " Failed to get info for A0A9Q6UCQ5\n",
      "\n",
      " Failed to get info for A0A9Q6UCQ5\n",
      "\n",
      " Failed to get info for A0A9Q6UCQ5\n",
      "Processed 200 in the last batch, total 10000\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Af6dse8&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 10200\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3Am0dfa8&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 10400\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/pfam/PF05935/?cursor=source%3As%3As0fuq1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 10600\n",
      "...Progress Saved!\n",
      "None\n",
      "Processed 141 in the last batch, total 10741\n",
      "...Progress Saved!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load sequences and identifiers from scratch!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 102\u001b[0m total_data \u001b[38;5;241m=\u001b[39m \u001b[43moutput_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 86\u001b[0m, in \u001b[0;36moutput_list\u001b[0;34m(next)\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...Progress Saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 86\u001b[0m       f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_data\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "def output_list(next = BASE_URL):\n",
    "  #disable SSL verification to avoid config issues\n",
    "  context = ssl._create_unverified_context()\n",
    "  last_page = False\n",
    "  total_data = []\n",
    "\n",
    "  if(next != BASE_URL):\n",
    "        print(\"Not starting from the beginning! Loading previous Json!\")\n",
    "        with open(output_filename, 'r') as f:\n",
    "            # load the data from the file\n",
    "            total_data = json.load(f)\n",
    "        print(f\"We have a total of {len(total_data)} sequences!\")\n",
    "        print(\"\\nContinuing where we left off!...\\n\")\n",
    "        \n",
    "  attempts = 0\n",
    "  while next:\n",
    "    try:\n",
    "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "      res = request.urlopen(req, context=context)\n",
    "      # If the API times out due a long running query\n",
    "      if res.status == 408:\n",
    "        # wait just over a minute\n",
    "        sleep(61)\n",
    "        # then continue this loop with the same URL\n",
    "        continue\n",
    "      elif res.status == 204:\n",
    "        #no data so leave loop\n",
    "        break\n",
    "      payload = json.loads(res.read().decode())\n",
    "      next = payload[\"next\"]\n",
    "      print(next)\n",
    "      \n",
    "      attempts = 0\n",
    "      if not next:\n",
    "        last_page = True\n",
    "    except HTTPError as e:\n",
    "      if e.code == 408:\n",
    "        sleep(61)\n",
    "        continue\n",
    "      else:\n",
    "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "        if attempts < 3:\n",
    "          attempts += 1\n",
    "          sleep(61)\n",
    "          continue\n",
    "        else:\n",
    "          sys.stderr.write(\"LAST URL: \" + next)\n",
    "          raise e\n",
    "    data = []\n",
    "    for i, item in enumerate(payload[\"results\"]):\n",
    "      entries = None\n",
    "      if (\"entry_subset\" in item):\n",
    "        entries = item[\"entry_subset\"]\n",
    "      elif (\"entries\" in item):\n",
    "        entries = item[\"entries\"]\n",
    "      \n",
    "      seq_data = {}\n",
    "      if entries is not None:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for entry in entries:\n",
    "          for locations in entry['entry_protein_locations']:\n",
    "            for fragment in locations['fragments']:\n",
    "              start = fragment['start']\n",
    "              end = fragment['end']\n",
    "        \n",
    "        seq_data[\"Accession_Interpro\"] = item[\"metadata\"][\"accession\"]\n",
    "        seq_data[\"domain_boundaries\"] = {\"start\":start, \"end\":end}\n",
    "      seq_data[\"seq\"] = item[\"extra_fields\"][\"sequence\"]\n",
    "      data.append(seq_data)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # submit a task to retrieve information for each accession\n",
    "        tasks = [executor.submit(get_additional_info, seq_data) for seq_data in data]\n",
    "\n",
    "        # retrieve the results of the tasks as they complete\n",
    "        results = [task.result() for task in concurrent.futures.as_completed(tasks)]\n",
    "\n",
    "    \n",
    "    total_data.extend(data)\n",
    "    print(f\"Processed {len(data)} in the last batch, total {len(total_data)}\")\n",
    "    # Don't overload the server, give it time before asking for more\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(total_data,f)\n",
    "        print(\"...Progress Saved!\")\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"\\n\"+next)\n",
    "        \n",
    "  return total_data\n",
    "print(\"Hello\")\n",
    "total_data = []\n",
    "url = BASE_URL\n",
    "if os.path.exists(installation_progress_filename):\n",
    "    print(\"Identified pre existing save, loading... \")\n",
    "    with open(installation_progress_filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            url = lines[-1]\n",
    "else:\n",
    "    print(\"Starting to load sequences and identifiers from scratch!\")\n",
    "    open(filename, \"w\").close()\n",
    "    \n",
    "total_data = output_list(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af229188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_data = []\n",
    "\n",
    "with open('../processed_sequences/annotated_sequences.json', 'r') as f:\n",
    "    total_data = json.load(f)\n",
    "#total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6be1b7",
   "metadata": {},
   "source": [
    "# Since the sequences have already been loaded, no need to run the previous script, just use the stuff below to reload the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []\n",
    "\n",
    "with open('../raw_sequences/annotated_sequences.json', 'r') as f:\n",
    "    total_data = json.load(f)\n",
    "#total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_list(data, result={}):\n",
    "    for item in data:\n",
    "        lineage = item[\"lineage\"]\n",
    "        if lineage:\n",
    "            lineage_dict = result\n",
    "            for level in lineage:\n",
    "                if level not in lineage_dict:\n",
    "                    lineage_dict[level] = {}\n",
    "                lineage_dict = lineage_dict[level]\n",
    "            lineage_dict.update(item)\n",
    "    return result\n",
    "\n",
    "transformed_data = transform_list(total_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3df89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(data, family):\n",
    "    return [s for s in data if family in s['lineage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Draw_Pie(data, depth = 2):\n",
    "    # Create a defaultdict to store the count of each lineage\n",
    "    lineage_count = defaultdict(int)\n",
    "\n",
    "    # Iterate through the list of dictionaries and count the occurrences of each lineage\n",
    "    for item in data:\n",
    "        try:\n",
    "            lineage_count[item[\"lineage\"][depth]] += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Extract the labels (lineage names) and sizes (counts) for the pie chart\n",
    "    labels = list(lineage_count.keys())\n",
    "    sizes = list(lineage_count.values())\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "    plt.axis('equal')  # Ensure the chart is a circle, not an ellipse\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803fd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Draw_Pie(total_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio as bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "\n",
    "family = \"Cyanobacteria\"\n",
    "filterByDomain = True\n",
    "\n",
    "sequences = get_sequences(total_data,family)\n",
    "seq_recs = []\n",
    "for seq_data in sequences:\n",
    "    seq = Seq(seq_data[\"seq\"])\n",
    "    if(filterByDomain):\n",
    "        seq = seq[seq_data[\"YcaO_domain\"][\"start\"]:seq_data[\"YcaO_domain\"][\"end\"]]\n",
    "    seq_recs.append(SeqRecord(seq, id=seq_data[\"Accession_Interpro\"]))\n",
    "\n",
    "dirname = f'../processed_sequences/{family}_sequences'\n",
    "filename = f\"{family}_{'YcaO_only' if filterByDomain else 'whole_protein'}.fa\"\n",
    "\n",
    "if not os.path.exists(dirname):\n",
    "    # Create the directory\n",
    "    os.makedirs(dirname)\n",
    "    \n",
    "SeqIO.write(seq_recs,os.path.join(dirname,filename), \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb27fb0",
   "metadata": {},
   "source": [
    "# Loading the tridomain YcaOs, basically using the interpro domain definition rather than anything fancy I define myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To restart the download run this cell!\n",
    "!rm ../processed_sequences/tridomain_url_progress.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tridomain_sequences_filename = '../processed_sequences/tridomain_annotated_sequences.json'\n",
    "with open(tridomain_sequences_filename, 'w') as f:\n",
    "    json.dump([],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"You have already run this code, sure you wanna do it again?\")\n",
    "\n",
    "BASE_URL = \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR022291/?page_size=200&extra_fields=sequence&taxonomy\"\n",
    "\n",
    "HEADER_SEPARATOR = \"|\"\n",
    "\n",
    "\n",
    "def output_list(next = BASE_URL):\n",
    "  #disable SSL verification to avoid config issues\n",
    "  context = ssl._create_unverified_context()\n",
    "  last_page = False\n",
    "  total_data = []\n",
    "\n",
    "  if(next != BASE_URL):\n",
    "        print(\"Not starting from the beginning! Loading previous Json!\")\n",
    "        with open(tridomain_sequences_filename, 'r') as f:\n",
    "            # load the data from the file\n",
    "            total_data = json.load(f)\n",
    "        print(f\"We have a total of {len(total_data)} sequences!\")\n",
    "        print(\"\\nContinuing where we left off!...\\n\")\n",
    "        \n",
    "  attempts = 0\n",
    "  while next:\n",
    "    try:\n",
    "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "      res = request.urlopen(req, context=context)\n",
    "      # If the API times out due a long running query\n",
    "      if res.status == 408:\n",
    "        # wait just over a minute\n",
    "        sleep(61)\n",
    "        # then continue this loop with the same URL\n",
    "        continue\n",
    "      elif res.status == 204:\n",
    "        #no data so leave loop\n",
    "        break\n",
    "      payload = json.loads(res.read().decode())\n",
    "      next = payload[\"next\"]\n",
    "      \n",
    "      attempts = 0\n",
    "      if not next:\n",
    "        last_page = True\n",
    "    except HTTPError as e:\n",
    "      if e.code == 408:\n",
    "        sleep(61)\n",
    "        continue\n",
    "      else:\n",
    "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "        if attempts < 3:\n",
    "          attempts += 1\n",
    "          sleep(61)\n",
    "          continue\n",
    "        else:\n",
    "          sys.stderr.write(\"LAST URL: \" + next)\n",
    "          raise e\n",
    "    data = []\n",
    "    for i, item in enumerate(payload[\"results\"]):\n",
    "      entries = None\n",
    "      if (\"entry_subset\" in item):\n",
    "        entries = item[\"entry_subset\"]\n",
    "      elif (\"entries\" in item):\n",
    "        entries = item[\"entries\"]\n",
    "      seq_data = {}\n",
    "      if entries is not None:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for entry in entries:\n",
    "          for locations in entry['entry_protein_locations']:\n",
    "            for fragment in locations['fragments']:\n",
    "              start = fragment['start']\n",
    "              end = fragment['end']\n",
    "        \n",
    "        seq_data[\"Accession_Interpro\"] = item[\"metadata\"][\"accession\"]\n",
    "        seq_data[\"Dehydratase_domain\"] = {\"start\":start, \"end\":end}\n",
    "      seq_data[\"seq\"] = item[\"extra_fields\"][\"sequence\"]\n",
    "      data.append(seq_data)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # submit a task to retrieve information for each accession\n",
    "        tasks = [executor.submit(get_additional_info, seq_data) for seq_data in data]\n",
    "\n",
    "        # retrieve the results of the tasks as they complete\n",
    "        results = [task.result() for task in concurrent.futures.as_completed(tasks)]\n",
    "\n",
    "    \n",
    "    total_data.extend(data)\n",
    "    print(f\"Processed {len(data)} in the last batch, total {len(total_data)} out of 5 000 sequences\")\n",
    "    # Don't overload the server, give it time before asking for more\n",
    "    with open(tridomain_sequences_filename, 'w') as f:\n",
    "        json.dump(total_data,f)\n",
    "        print(\"...Progress Saved!\")\n",
    "    \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"\\n\"+next)\n",
    "        \n",
    "  return total_data\n",
    "\n",
    "filename = \"../processed_sequences/tridomain_url_progress.txt\"\n",
    "total_data = []\n",
    "url = BASE_URL\n",
    "if os.path.exists(filename):\n",
    "    print(\"Identified pre existing save, loading... \")\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            url = lines[-1]\n",
    "else:\n",
    "    print(\"Starting to load sequences and identifiers from scratch!\")\n",
    "    open(filename, \"w\").close()\n",
    "    \n",
    "total_data = output_list(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "YcaO_data = []\n",
    "with open('../raw_sequences/annotated_sequences.json', 'r') as f:\n",
    "    YcaO_data = json.load(f)\n",
    "    \n",
    "    \n",
    "E1_data = []\n",
    "\n",
    "with open('../raw_sequences/cyclodehydratase_annotaded_sequences.json', 'r') as f:\n",
    "    E1_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1_subset = []\n",
    "E1_containing_accessions = [e1[\"Accession_Interpro\"] for e1 in E1_data]\n",
    "for ycao in YcaO_data:\n",
    "    if ycao[\"Accession_Interpro\"] in E1_containing_accessions:\n",
    "        E1_subset.append(ycao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(E1_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395147c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

